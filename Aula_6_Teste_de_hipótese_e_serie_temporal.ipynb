{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHEgMGY4gMAL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "    'H0': 'Hipótese Nula - Declaração inicial que não há efeito ou diferença.',\n",
        "    'H1': 'Hipótese Alternativa - Declaração que contradiz a hipótese nula, sugerindo um efeito ou diferença.',\n",
        "    'α': 'Nível de Significância - Probabilidade de rejeitar a hipótese nula quando é verdadeira (erro do Tipo I).',\n",
        "    'β': 'Probabilidade do Erro do Tipo II - Probabilidade de não rejeitar a hipótese nula quando a hipótese alternativa é verdadeira.',\n",
        "    'Teste Unilateral': 'Teste de hipótese que rejeita a hipótese nula em uma direção (superior ou inferior).',\n",
        "    'Teste Bilateral': 'Teste de hipótese que rejeita a hipótese nula em ambas as direções.',\n",
        "    'Valor Crítico': 'Limite além do qual a hipótese nula é rejeitada.',\n",
        "    'P-valor': 'Probabilidade de observar um resultado tão extremo quanto (ou mais extremo do que) o observado, dado que a hipótese nula é verdadeira.',\n",
        "    'Rejeitar H0': 'Decisão de rejeitar a hipótese nula com base nos resultados do teste.',\n",
        "    'Aceitar H0': 'Decisão de não rejeitar a hipótese nula com base nos resultados do teste.',\n",
        "    'Estatística de Teste': 'Valor calculado que é usado para decidir se rejeitar ou não a hipótese nula.'\n"
      ],
      "metadata": {
        "id": "woUezgOW5q0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os testes de hipótese são procedimentos estatísticos utilizados para tomar decisões sobre uma população com base em uma amostra dos dados dessa população. Esses testes são fundamentais no campo da estatística inferencial, que busca fazer inferências sobre uma população com base em informações obtidas de uma amostra representativa.\n",
        "\n",
        "O processo de teste de hipótese geralmente envolve a formulação de duas hipóteses: a hipótese nula (H0) e a hipótese alternativa (H1). A hipótese nula é uma afirmação que assume que não há efeito ou diferença significativa, enquanto a hipótese alternativa sugere que existe um efeito ou diferença significativa.\n",
        "\n",
        "O teste de hipótese é conduzido através da coleta de dados e cálculo de uma estatística de teste. Essa estatística é então comparada a um valor crítico ou p-value para determinar se há evidências estatísticas suficientes para rejeitar a hipótese nula em favor da hipótese alternativa.\n",
        "\n",
        "Em resumo, os testes de hipótese são ferramentas estatísticas cruciais que permitem aos cientistas de dados e pesquisadores tomar decisões informadas sobre as características de uma população com base em dados amostrais, contribuindo assim para a validade e confiabilidade das conclusões estatísticas."
      ],
      "metadata": {
        "id": "Tvc0gneWgrMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FORMULAÇÃO DO PROBLEMA: #\n",
        "\n",
        "Será que as médias dos valores cobrados são estatisticamente significativas entre todas as cidades?"
      ],
      "metadata": {
        "id": "2HBO-aR9h6h5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Passo 1** - Formulação das hipóteses $H_0$ e $H_1$\n",
        "\n",
        "> ### <font color='red'>Pontos importantes</font>\n",
        "> - A hipótese nula sempre afirma uma igualdade ou propriedade populacional, e $H_1$ a desigualdade que nega $H_0$.\n",
        "> - No caso da hipótese nula $H_0$ a igualdade pode ser representada por uma igualdade simples \"$=$\" ou por \"$\\geq$\" e \"$\\leq$\". Sempre complementar ao estabelecido pela hipótese alternativa.\n",
        "> - A hipótese alternativa $H_1$ deve definir uma desigualdade que pode ser uma diferença simples \"$\\neq$\" ou dos tipos \"$>$\" e \"$<$\".\n",
        "\n",
        "\n",
        "**No nosso problema temos uma condição normal (comum) e uma hipótese:**\n",
        "    \n",
        "$\\rightarrow$ condição: variável valor cobrado tem média= 86,48 reais e variância=487445$^2$\n",
        "\n",
        "$\\rightarrow$ hipótese inicial: todas as cidades possuem a mesma média\n",
        "\n",
        "$\\rightarrow$ hipótese alternativa: as cidades possuem médias diferentes"
      ],
      "metadata": {
        "id": "cZqQ_9Koh3za"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Passo 2** - Escolha da distribuição amostral adequada\n",
        "\n",
        "> ### <font color='red'>Pontos importantes</font>\n",
        "> - Quando o tamanho da amostra tiver 30 elementos ou mais, deve-se utilizar a distribuição normal, como estabelecido pelo **teorema do limite central**.\n",
        "> - Para um tamanho de amostra menor que 30 elementos, e se pudermos afirmar que a população se distribui aproximadamente como uma normal e o desvio padrão populacional for conhecido, deve-se utilizar a distribuição normal.\n",
        "> - Para um tamanho de amostra menor que 30 elementos, e se pudermos afirmar que a população se distribui aproximadamente como uma normal e o desvio padrão populacional não for conhecido, deve-se utilizar a distribuição t de Student."
      ],
      "metadata": {
        "id": "UhsecfHllLqe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Passo 3** - Fixação da significância do teste ($\\alpha$), que define as regiões de aceitação e rejeição das hipóteses (os valores mais freqüentes são 10%, 5% e 1%);\n",
        "\n",
        "> ### <font color='red'>Pontos importantes</font>\n",
        "> - O **nível de confiança** ($1 - \\alpha$) representa a probabilidade de acerto da estimativa. De forma complementar o **nível de significância** ($\\alpha$) expressa a probabilidade de erro da estimativa.\n",
        ">\n",
        "> <img alt=\"Níveis de Confiança e significância\" src=\"https://caelum-online-public.s3.amazonaws.com/1229-estatistica-parte3/01/img001.png\" style=\"display: block; margin: 25px auto\" />\n",
        ">\n",
        "> - O **nível de confiança** representa o grau de confiabilidade do resultado da estimativa estar dentro de determinado intervalo. Quando fixamos em uma pesquisa um **nível de confiança** de 95%, por exemplo, estamos assumindo que existe uma probabilidade de 95% dos resultados da pesquisa representarem bem a realidade, ou seja, estarem corretos.\n",
        ">\n",
        "> <img alt=\"Áreas de Aceitação e Rejeição\" src=\"https://caelum-online-public.s3.amazonaws.com/1229-estatistica-parte3/01/img002.png\" style=\"display: block; margin: 25px auto; border-radius: 10px\" />\n",
        "\n",
        "Vamos usar 5%, isso quer dizer que vamos considerar como resultado \"comum\", \"evento comum\", valores de média de amostra que acontecem até 95% das vezes.\n",
        "\n",
        "Vamos considerar que os valores nos 5% das caudas serão considerados eventos muito raros para nossa amostra e vamos considerar como um sinal de que há algo de errado com a média."
      ],
      "metadata": {
        "id": "JwfZudMglulj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Passo 4** - cálculo da estatística-teste e verificação desse valor com as áreas de aceitação e rejeição do teste;\n",
        "\n",
        "> ### <font color='red'>Pontos importantes</font>\n",
        "> - Nos testes paramétricos, distância relativa entre a estatística amostral e o valor alegado como provável.\n",
        "> - Neste passo são obtidas as estatísticas amostrais necessárias à execução do teste (média, desvio-padrão, graus de liberdade etc.)\n",
        "\n",
        "Relembrando os valores que temos:\n",
        "\n",
        "média = 86,48, desvio = 698.17\n",
        "\n",
        "significância = 5%\n",
        "\n",
        "H1 do tipo \"diferente\"\n",
        "\n",
        "Vamos descobrir o intervalo que determina os 95%:"
      ],
      "metadata": {
        "id": "xOtPVB8smdBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A função `norm.ppf` é parte do módulo `scipy.stats` e é utilizada para calcular o percentil de uma distribuição normal inversa, também conhecida como o quantil. Essa função é usada principalmente para encontrar valores críticos associados a uma distribuição normal padrão (média 0, desvio padrão 1), especificando um percentil desejado.\n",
        "\n",
        "A assinatura da função é a seguinte:\n",
        "```python\n",
        "scipy.stats.norm.ppf(q, loc=0, scale=1)\n",
        "```\n",
        "\n",
        "- `q`: Percentil desejado.\n",
        "- `loc`: Média da distribuição (padrão é 0).\n",
        "- `scale`: Desvio padrão da distribuição (padrão é 1).\n",
        "\n",
        "No contexto do cálculo do intervalo de confiança, a expressão `norm.ppf(1 - significancia/2, loc=uo, scale=sigma)` está sendo usada para encontrar o valor crítico associado ao percentil `(1 - significancia/2)`, onde `significancia` é a significância estatística (nível de confiança).\n",
        "\n",
        "Entendendo cada parte:\n",
        "\n",
        "- `(1 - significancia/2)`: Este é o percentil superior do intervalo de confiança. Por exemplo, se você estiver usando um nível de confiança de 95%, isso será `0.975`, indicando que você deseja abranger 97.5% da área sob a curva de uma distribuição normal padrão.\n",
        "\n",
        "- `loc=uo`: Este é o parâmetro de localização, que representa a média da distribuição normal. No contexto do intervalo de confiança, `uo` é frequentemente interpretado como a média populacional.\n",
        "\n",
        "- `scale=sigma`: Este é o parâmetro de escala, que representa o desvio padrão da distribuição normal. No contexto do intervalo de confiança, `sigma` geralmente é o desvio padrão populacional.\n",
        "\n",
        "Portanto, `norm.ppf(1 - significancia/2, loc=uo, scale=sigma)` retorna o valor crítico associado ao percentil superior do intervalo de confiança desejado, considerando a média e o desvio padrão fornecidos. Isso é útil para estabelecer os limites superiores do intervalo de confiança ao calcular estatísticas em torno de uma média populacional."
      ],
      "metadata": {
        "id": "cW1PhVZyqXPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import norm\n",
        "\n",
        "def teste_z_para_coluna(dataframe, coluna, significancia=0.05):\n",
        "    # Obtendo os parâmetros da coluna\n",
        "    media_amostral = dataframe[coluna].mean()\n",
        "    mi = dataframe[coluna].mean()\n",
        "    sigma = dataframe[coluna].std()\n",
        "    n = len(dataframe[coluna])\n",
        "\n",
        "    # Calculando os limites do intervalo de confiança\n",
        "    maximo = norm.ppf(1 - significancia/2, loc=mi, scale=sigma)\n",
        "    minimo = norm.ppf(significancia/2, loc=mi, scale=sigma)\n",
        "\n",
        "    # Calculando a estatística Z\n",
        "    z = (media_amostral - mi) / (sigma / n**0.5)\n",
        "\n",
        "    # Verificando se a estatística de teste está dentro do intervalo de aceitação\n",
        "    if minimo <= z <= maximo:\n",
        "        resultado = \"Aceitar H0\"\n",
        "    else:\n",
        "        resultado = \"Rejeitar H0\"\n",
        "\n",
        "    # Retornando os resultados\n",
        "    return {\n",
        "        \"Estatística Z\": z,\n",
        "        \"Intervalo de Confiança\": [minimo, maximo],\n",
        "        \"Resultado do Teste\": resultado\n",
        "    }\n",
        "\n",
        "coluna_interesse = 'VR_COBRADO'\n",
        "\n",
        "# Chamando a função\n",
        "resultado_teste = teste_z_para_coluna(df, coluna_interesse)\n",
        "print(resultado_teste)\n",
        "\n"
      ],
      "metadata": {
        "id": "kQYWIiYBgxLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Passo 5** - Aceitação ou rejeição da hipótese nula.\n",
        "\n",
        "> ### <font color='red'>Pontos importantes</font>\n",
        "> - No caso de o intervalo de aceitação conter a estatística-teste, aceita-se $H_0$ como estatisticamente válido e rejeita-se $H_1$ como tal.\n",
        "> - No caso de o intervalo de aceitação não conter a estatística-teste, rejeita-se $H_0$ e aceita-se $H_1$ como provavelmente verdadeira.\n",
        "> - A aceitação também se verifica com a probabilidade de cauda (p-valor): se maior que $\\alpha$, aceita-se $H_0$.\n"
      ],
      "metadata": {
        "id": "ReVs4pxXq8aB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "   plt.figure(figsize=(12, 8))\n",
        "    ax = sns.scatterplot(x='NM_UNIMED', y='VR_DESP_REAL', data=df)\n",
        "    ax.set_title('Gráfico de Dispersão de NM_UNIMED por VR_DESP_REAL')\n",
        "\n",
        "    # Oculta as etiquetas do eixo x\n",
        "    ax.set_xticklabels([])\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "vwPrHCNHC6Ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "def teste_anova_unimeds(df, coluna_unimed, coluna_variavel, nivel_significancia=0.05):\n",
        "\n",
        "    # Obtém as Unimeds únicas\n",
        "    unimeds_unicas = df[coluna_unimed].unique()\n",
        "\n",
        "    # Prepara os argumentos para o teste ANOVA\n",
        "    argumentos_anova = [df[coluna_variavel][df[coluna_unimed] == unimed] for unimed in unimeds_unicas]\n",
        "\n",
        "    # Realiza o teste de ANOVA\n",
        "    resultado_anova = f_oneway(*argumentos_anova)\n",
        "\n",
        "    # Exibe o resultado do teste\n",
        "    print(\"\\nResultado do Teste ANOVA:\")\n",
        "    print(resultado_anova)\n",
        "\n",
        "    # Verifica se o p-valor é menor que o nível de significância\n",
        "    if resultado_anova.pvalue < nivel_significancia:\n",
        "        print(\"\\nConclusão: As médias das Unimeds são estatisticamente diferentes.\")\n",
        "    else:\n",
        "        print(\"\\nConclusão: Não há evidências para afirmar que as médias das Unimeds são diferentes.\")\n",
        "\n",
        "\n",
        "# Chama a função com as colunas relevantes\n",
        "teste_anova_unimeds(df, coluna_unimed='NM_UNIMED', coluna_variavel='VR_DESP_REAL')\n"
      ],
      "metadata": {
        "id": "hbYoDyJ0rDNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SERIE TEMPORAL"
      ],
      "metadata": {
        "id": "VMIxXT-HG3EI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Série Temporal: Uma Visão Geral**\n",
        "\n",
        "Uma série temporal é uma sequência ordenada de observações de uma variável ao longo do tempo. Essa variável pode representar qualquer coisa que seja mensurável, como temperatura, vendas, preço de ações, ou qualquer outra grandeza que varie conforme as mudanças no tempo.\n",
        "\n",
        "**Principais Características:**\n",
        "\n",
        "1. **Ordenação Temporal:** As observações em uma série temporal são organizadas cronologicamente, geralmente em intervalos regulares, como dias, meses ou anos. A ordem temporal é fundamental para entender as tendências e padrões ao longo do tempo.\n",
        "\n",
        "2. **Dependência Temporal:** Diferentemente de dados transversais (cross-sectional data), onde cada observação é independente uma da outra, as observações em uma série temporal estão inter-relacionadas. O valor em um determinado momento muitas vezes depende do valor em momentos anteriores.\n",
        "\n",
        "**Componentes Principais de uma Série Temporal:**\n",
        "\n",
        "1. **Tendência:** Refere-se à direção geral da série ao longo do tempo. Pode ser ascendente, descendente ou manter-se constante.\n",
        "\n",
        "2. **Sazonalidade:** Refere-se a padrões que se repetem em ciclos fixos. Pode ser diária, mensal, anual, ou seguir outros padrões recorrentes.\n",
        "\n",
        "3. **Ciclo:** Refere-se a padrões que se repetem, mas não necessariamente com frequência fixa. Os ciclos geralmente são mais longos do que os padrões sazonais.\n",
        "\n",
        "4. **Componente Residual (ou Aleatório):** Refere-se à variação não explicada pelos componentes anteriores. Pode incluir eventos imprevisíveis e ruídos.\n",
        "\n",
        "**Aplicações de Análise de Séries Temporais:**\n",
        "\n",
        "1. **Previsão:** Utilizada para prever valores futuros com base em padrões identificados na série temporal.\n",
        "\n",
        "2. **Monitoramento:** Aplicada para monitorar e entender mudanças ao longo do tempo, como o desempenho de vendas, a evolução de uma doença, entre outros.\n",
        "\n",
        "3. **Identificação de Padrões e Anomalias:** Ajuda a identificar tendências emergentes, sazonalidades, ou eventos extraordinários na série temporal.\n",
        "\n",
        "4. **Tomada de Decisões:** Fornece informações valiosas para orientar decisões estratégicas com base nas tendências e padrões observados.\n",
        "\n",
        "**Ferramentas Estatísticas e Modelos:**\n",
        "\n",
        "1. **Suavização Exponencial:** Método que atribui diferentes pesos a observações recentes, dando mais importância a valores mais recentes.\n",
        "\n",
        "2. **ARIMA (Modelo Autorregressivo Integrado de Médias Móveis):** Modelo estatístico que leva em conta autocorrelações e tendências.\n",
        "\n",
        "3. **Regressão Linear Temporal:** Utiliza uma abordagem linear para modelar a relação entre variáveis e tempo.\n",
        "\n",
        "4. **Redes Neurais e Machine Learning:** Abordagens mais avançadas que podem capturar padrões complexos em séries temporais.\n",
        "\n",
        "Em resumo, a análise de séries temporais desempenha um papel crucial em diversos campos, fornecendo insights valiosos para entender o passado, prever o futuro e tomar decisões informadas.\n",
        "\n",
        "Algumas ilustrações:\n",
        "\n",
        "\n",
        "<img src=\"https://i2.wp.com/itfeature.com/wp-content/uploads/2014/06/Component-of-Time-Series-Data.jpg?resize=661%2C599\" width=600>"
      ],
      "metadata": {
        "id": "ys-7Tg93G8TB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sales = df.groupby('DT_COMPET')['VR_DESP_REAL'].sum().reset_index()\n",
        "# Converter a coluna 'DT_COMPET' para datetime, se necessário\n",
        "df_sales['DT_COMPET'] = pd.to_datetime(df_sales['DT_COMPET'])\n",
        "\n",
        "# Configurar 'DT_COMPET' como o índice (necessário para a decomposição)\n",
        "df_sales.set_index('DT_COMPET', inplace=True)"
      ],
      "metadata": {
        "id": "1mLugN1wG6j_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 5))\n",
        "sns.lineplot(x='DT_COMPET', y='VR_DESP_REAL', data=df_sales)\n",
        "plt.xlabel('Dias')\n",
        "plt.ylabel('Quantidade de Vendas')\n",
        "plt.title('Quantidade de Vendas por Dia')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xfa84HSnJOMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "\n",
        "decomposed = seasonal_decompose(df_sales, model='additive', period=12)\n",
        "\n",
        "fig = decomposed.plot()\n",
        "fig.set_size_inches((15, 10))\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0eh7NJqAJYJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.api import ExponentialSmoothing\n",
        "\n",
        "# Ajustar o modelo Exponential Smoothing (Holt-Winters)\n",
        "modelo = ExponentialSmoothing(df['quantidade'], seasonal='add', seasonal_periods=12)\n",
        "resultado = modelo.fit()\n",
        "\n",
        "# Realize a previsão para os próximos 7 dias\n",
        "previsao = resultado.forecast(steps=7)\n",
        "\n",
        "# Visualize os resultados\n",
        "plt.figure(figsize=(15, 8))\n",
        "plt.plot(df_prev.index, df['quantidade'], label='Observado')\n",
        "plt.plot(previsao.index, previsao, label='Previsão', linestyle='dashed', color='orange')\n",
        "plt.xlabel('Data')\n",
        "plt.ylabel('Quantidade de Vendas')\n",
        "plt.title('Previsão de Vendas para os Próximos 7 Dias')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "AUVALbypOivz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}